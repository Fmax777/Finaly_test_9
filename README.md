## Итоговое задание №9
Есть csv файлы, расположенные по [ссылке](https://disk.yandex.ru/d/TXZjQ6bbuWCo_g). Скачивайте. Там внутри 3 файла. 

Так как работа дата инженера не всегда легка и проста, то иногда приходится разбираться с тем, какие данные находятся в файлах. Но, схему мы Вам все таки дадим :)

##### Файл с данными о полетах:

* DATE: Дата полета.
* DAY_OF_WEEK: День недели.
* AIRLINE: Авиалиния.
* FLIGHT_NUMBER: Номер рейса.
* TAIL_NUMBER: Номер самолета.
* ORIGIN_AIRPORT: Аэропорт отправления (IATA код).
* DESTINATION_AIRPORT: Аэропорт назначения (IATA код).
* DEPARTURE_DELAY: Задержка при вылете (в минутах).
* DISTANCE: Расстояние (в милях).
* ARRIVAL_DELAY: Задержка при прибытии (в минутах).
* DIVERTED: Был ли рейс перенаправлен.
* CANCELLED: Был ли рейс отменен.
* CANCELLATION_REASON: Причина отмены (A - авиалиния, B - погода, C - NAS, D - безопасность).
* AIR_SYSTEM_DELAY: Задержка по вине системы авиадиспетчерского управления.
* SECURITY_DELAY: Задержка по причинам безопасности.
* AIRLINE_DELAY: Задержка по вине авиалинии.
* LATE_AIRCRAFT_DELAY: Задержка по вине позднего прибытия предыдущего рейса.
* WEATHER_DELAY: Задержка из-за погоды.
* DEPARTURE_HOUR: Час вылета.
* ARRIVAL_HOUR: Час прибытия.

##### Файл с данными о аэропортах:

* IATA CODE: Код IATA.
* Airport: Название аэропорта.
* City: Город.
* Latitude: Широта.
* Longitude: Долгота.

##### Файл с данными о авиалиниях:

* IATA CODE: Код IATA.
* AIRLINE: Название авиалинии.

А что, если мы (аналитик) все же ошиблись/ся с названиями или определениями полей? Это необходимо проверить!

Что необходимо сделать? Начнем с простого.

 

##### -1. Обратите внимание, что в проекте используются PySpark и Clickhouse. PySpark в airflow в docker-compose нету! Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная. 

1. Предварительно разверните docker-compose из первых степов! Шаги 1-7 должны быть сделаны в DAG.

2. Загрузите файл данных в DataFrame PySpark. Обязательно выведите количество строк.

3. Убедитесь, что данные корректно прочитаны (правильный формат, отсутствие пустых строк).

4. Преобразуйте текстовые и числовые поля в соответствующие типы данных (например, дата, число).

5. Найдите топ-5 авиалиний с наибольшей средней задержкой.

6. Вычислите процент отмененных рейсов для каждого аэропорта.

7. Определите, какое время суток (утро, день, вечер, ночь) чаще всего связано с задержками рейсов.

8. Добавьте в данные о полетах новые столбцы, рассчитанные на основе существующих данных:

   * IS_LONG_HAUL: Флаг, указывающий, является ли рейс дальнемагистральным (если расстояние больше 1000 миль).
   * DAY_PART: Определите, в какое время суток происходит вылет (утро, день, вечер, ночь).

##### С данными поработали, проанализировали, теперь можно их и в БД положить. 

9. Создайте схему таблицы в PostgreSQL, которая будет соответствовать структуре ваших данных. PostgreSQL уже находится в docker-compose! Схему нужно создать вне Airflow, например через Dbeaver.

10. Настройте соединение с PostgreSQL из кода, но из PySpark. (обязательно сделать это нужно в Airflow)

11. Загрузите только 10.000 строк из DataFrame в таблицу в PostgreSQL. (обязательно сделать это нужно в Airflow)

12. Выполните SQL скрипт в Python-PySpark скрипте, который выведет компанию - общее время полетов ее самолетов.

##### В качестве решения необходимо прислать ссылку на репозиторий. Обратите внимание на критерии (их 4) сдачи проекта - 

1. **Оформление.** Репозиторий оформлен так, как нужно. А именно есть структура, удобное содержание, читаемый README, естественно наличие задания. Обязательно наличие Docker файла.

2. **Логика.** Все сделано в одном скрипте. Сначала читаем данные с csv, потом преобразуем данные, анализируем, загружаем в postgresql, читаем с postgresql.

3. **Содержание.** Файл c csv хранить в GIT нельзя! Добавляйте либо ссылку, либо срез данных.

4. **Корректность.** Данные нельзя редактировать, файл нельзя обрезать. Результаты аналитики и SQL скрипта должны быть правильными.

### Использование:
Клонируем себе репозиторий. Скачиваем файлы с данными по [ссылке](https://disk.yandex.ru/d/TXZjQ6bbuWCo_g). Файлы необходимо поместить в папку data. Dag находится уже в папке dag. После запуска Dockera собираем Dockerfile командой "docker build -t airflow-with-java .", после собираем docker-compose командой "docker-compose up". После успешного запуска необходимо открыть DBeaver или pgAdmin 4, создать подключение к БД.
Данные для подключения:

	Хост - localhost
	Порт - 5432
	База данных - test
	Пользователь - user
	Пароль - password

Открыть готовый скрипт "Script_create_tables.sql" для создания таблиц и запустить Далее открываем [Airflow](http://localhost:8080), будет 2 dag файла, первым запускаем "01_init_connections.py", он производит автоматическую настройку подключений для PostgreSQL. Вторым запускаем "main.py". Будет выполнено все необходимое по заданию.

Структура файлов и папок на начальном этапе у вас должна получиться:

	project/
	├── dags/
	│   ├── 01_init_connections.py
	│   ├── connections.json
	│   └── main.py
	├── data/
	│   ├── airlines.csv
	│   ├── airports.csv
	│   └── flights_pak.csv
	├── Dockerfile-airflow
	├── docker-compose.yml
	├── README.md
	├── requirements.txt
	└── Script_create_tables.sql